"""
è¯„ä¼°æŒ‡æ ‡è®¡ç®—æ¨¡å—

å®ç° R@K (Recall at K) å’Œ mAP (mean Average Precision) ç­‰æŒ‡æ ‡
"""

import numpy as np
from typing import List, Dict, Set, Tuple
from collections import defaultdict


def compute_recall_at_k(
    retrieved_results: List[List[str]],
    ground_truth: List[Set[str]],
    k_values: List[int] = [1, 5, 10],
) -> Dict[int, float]:
    """
    è®¡ç®— Recall@K

    Args:
        retrieved_results: æŸ¥è¯¢ç»“æœåˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ æ˜¯ Top-K ç»“æœçš„è§†é¢‘IDåˆ—è¡¨
        ground_truth: çœŸå®æ ‡ç­¾é›†åˆåˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ æ˜¯è¯¥æŸ¥è¯¢çš„æ­£ç¡®è§†é¢‘IDé›†åˆ
        k_values: è¦è®¡ç®—çš„ K å€¼åˆ—è¡¨

    Returns:
        å„ä¸ª K å€¼çš„ Recall@K
    """
    recall_scores = {k: [] for k in k_values}

    for results, gt in zip(retrieved_results, ground_truth):
        for k in k_values:
            top_k = results[:k]
            num_relevant = len(set(top_k) & gt)
            recall = num_relevant / len(gt) if len(gt) > 0 else 0.0
            recall_scores[k].append(recall)

    # è®¡ç®—å¹³å‡ Recall
    avg_recall = {k: np.mean(scores) for k, scores in recall_scores.items()}

    return avg_recall


def compute_precision_at_k(
    retrieved_results: List[List[str]],
    ground_truth: List[Set[str]],
    k_values: List[int] = [1, 5, 10],
) -> Dict[int, float]:
    """
    è®¡ç®— Precision@K

    Args:
        retrieved_results: æŸ¥è¯¢ç»“æœåˆ—è¡¨
        ground_truth: çœŸå®æ ‡ç­¾é›†åˆåˆ—è¡¨
        k_values: è¦è®¡ç®—çš„ K å€¼åˆ—è¡¨

    Returns:
        å„ä¸ª K å€¼çš„ Precision@K
    """
    precision_scores = {k: [] for k in k_values}

    for results, gt in zip(retrieved_results, ground_truth):
        for k in k_values:
            top_k = results[:k]
            num_relevant = len(set(top_k) & gt)
            precision = num_relevant / k if k > 0 else 0.0
            precision_scores[k].append(precision)

    # è®¡ç®—å¹³å‡ Precision
    avg_precision = {k: np.mean(scores) for k, scores in precision_scores.items()}

    return avg_precision


def compute_average_precision(
    retrieved_results: List[str], ground_truth: Set[str]
) -> float:
    """
    è®¡ç®— Average Precision (AP)

    Args:
        retrieved_results: å•ä¸ªæŸ¥è¯¢çš„æ’åºç»“æœ
        ground_truth: è¯¥æŸ¥è¯¢çš„çœŸå®æ ‡ç­¾é›†åˆ

    Returns:
        Average Precision å€¼
    """
    if not ground_truth:
        return 0.0

    precisions = []
    num_relevant = 0

    for i, item in enumerate(retrieved_results):
        if item in ground_truth:
            num_relevant += 1
            precision = num_relevant / (i + 1)
            precisions.append(precision)

    if not precisions:
        return 0.0

    ap = np.mean(precisions)
    return ap


def compute_mean_average_precision(
    retrieved_results: List[List[str]], ground_truth: List[Set[str]]
) -> float:
    """
    è®¡ç®— mean Average Precision (mAP)

    Args:
        retrieved_results: æŸ¥è¯¢ç»“æœåˆ—è¡¨
        ground_truth: çœŸå®æ ‡ç­¾é›†åˆåˆ—è¡¨

    Returns:
        mAP å€¼
    """
    aps = []

    for results, gt in zip(retrieved_results, ground_truth):
        ap = compute_average_precision(results, gt)
        aps.append(ap)

    mAP = np.mean(aps)
    return mAP


def compute_ndcg_at_k(
    retrieved_results: List[List[str]],
    ground_truth: List[Set[str]],
    k_values: List[int] = [5, 10],
) -> Dict[int, float]:
    """
    è®¡ç®— NDCG@K (Normalized Discounted Cumulative Gain)

    Args:
        retrieved_results: æŸ¥è¯¢ç»“æœåˆ—è¡¨
        ground_truth: çœŸå®æ ‡ç­¾é›†åˆåˆ—è¡¨
        k_values: è¦è®¡ç®—çš„ K å€¼åˆ—è¡¨

    Returns:
        å„ä¸ª K å€¼çš„ NDCG@K
    """
    ndcg_scores = {k: [] for k in k_values}

    for results, gt in zip(retrieved_results, ground_truth):
        for k in k_values:
            top_k = results[:k]

            # è®¡ç®— DCG
            dcg = 0.0
            for i, item in enumerate(top_k):
                relevance = 1 if item in gt else 0
                dcg += relevance / np.log2(i + 2)

            # è®¡ç®— Ideal DCG
            idcg = 0.0
            num_relevant = min(len(gt), k)
            for i in range(num_relevant):
                idcg += 1 / np.log2(i + 2)

            # è®¡ç®— NDCG
            ndcg = dcg / idcg if idcg > 0 else 0.0
            ndcg_scores[k].append(ndcg)

    # è®¡ç®—å¹³å‡ NDCG
    avg_ndcg = {k: np.mean(scores) for k, scores in ndcg_scores.items()}

    return avg_ndcg


def compute_latency_stats(latencies: List[float]) -> Dict[str, float]:
    """
    è®¡ç®—å»¶è¿Ÿç»Ÿè®¡ä¿¡æ¯

    Args:
        latencies: å»¶è¿Ÿåˆ—è¡¨ï¼ˆæ¯«ç§’ï¼‰

    Returns:
        ç»Ÿè®¡ä¿¡æ¯å­—å…¸
    """
    if not latencies:
        return {}

    return {
        "mean": np.mean(latencies),
        "median": np.median(latencies),
        "std": np.std(latencies),
        "min": np.min(latencies),
        "max": np.max(latencies),
        "p95": np.percentile(latencies, 95),
        "p99": np.percentile(latencies, 99),
    }


def evaluate_search_system(
    queries: List[str],
    retrieved_results: List[List[str]],
    ground_truth: List[Set[str]],
    latencies: List[float],
) -> Dict[str, any]:
    """
    ç»¼åˆè¯„ä¼°æœç´¢ç³»ç»Ÿæ€§èƒ½

    Args:
        queries: æŸ¥è¯¢åˆ—è¡¨
        retrieved_results: æ£€ç´¢ç»“æœåˆ—è¡¨
        ground_truth: çœŸå®æ ‡ç­¾åˆ—è¡¨
        latencies: å»¶è¿Ÿåˆ—è¡¨

    Returns:
        è¯„ä¼°ç»“æœå­—å…¸
    """
    evaluation_results = {}

    # Recall@K
    recall_scores = compute_recall_at_k(retrieved_results, ground_truth)
    evaluation_results["recall"] = recall_scores

    # Precision@K
    precision_scores = compute_precision_at_k(retrieved_results, ground_truth)
    evaluation_results["precision"] = precision_scores

    # mAP
    mAP = compute_mean_average_precision(retrieved_results, ground_truth)
    evaluation_results["mAP"] = mAP

    # NDCG@K
    ndcg_scores = compute_ndcg_at_k(retrieved_results, ground_truth)
    evaluation_results["ndcg"] = ndcg_scores

    # å»¶è¿Ÿç»Ÿè®¡
    latency_stats = compute_latency_stats(latencies)
    evaluation_results["latency"] = latency_stats

    # æŸ¥è¯¢æ•°é‡
    evaluation_results["num_queries"] = len(queries)

    return evaluation_results


def print_evaluation_report(evaluation_results: Dict[str, any]):
    """
    æ‰“å°è¯„ä¼°æŠ¥å‘Š

    Args:
        evaluation_results: è¯„ä¼°ç»“æœå­—å…¸
    """
    print("=" * 60)
    print("ğŸ“Š è§†é¢‘æœç´¢å¼•æ“è¯„ä¼°æŠ¥å‘Š")
    print("=" * 60)

    # Recall@K
    print("\nğŸ“ˆ Recall@K:")
    for k, score in evaluation_results["recall"].items():
        print(f"   R@{k}: {score:.4f}")

    # Precision@K
    print("\nğŸ“ˆ Precision@K:")
    for k, score in evaluation_results["precision"].items():
        print(f"   P@{k}: {score:.4f}")

    # mAP
    print(f"\nğŸ“ˆ Mean Average Precision (mAP): {evaluation_results['mAP']:.4f}")

    # NDCG@K
    print("\nğŸ“ˆ NDCG@K:")
    for k, score in evaluation_results["ndcg"].items():
        print(f"   NDCG@{k}: {score:.4f}")

    # å»¶è¿Ÿ
    if evaluation_results.get("latency"):
        print("\nâ±ï¸  å»¶è¿Ÿç»Ÿè®¡ (æ¯«ç§’):")
        latency = evaluation_results["latency"]
        print(f"   å¹³å‡: {latency['mean']:.2f}")
        print(f"   ä¸­ä½æ•°: {latency['median']:.2f}")
        print(f"   æ ‡å‡†å·®: {latency['std']:.2f}")
        print(f"   æœ€å°: {latency['min']:.2f}")
        print(f"   æœ€å¤§: {latency['max']:.2f}")
        print(f"   P95: {latency['p95']:.2f}")
        print(f"   P99: {latency['p99']:.2f}")

    print(f"\nğŸ“ æŸ¥è¯¢æ•°é‡: {evaluation_results.get('num_queries', 0)}")
    print("=" * 60)


if __name__ == "__main__":
    # æµ‹è¯•ç¤ºä¾‹
    test_retrieved = [
        ["video1", "video2", "video3", "video4", "video5"],
        ["video2", "video1", "video3", "video4", "video5"],
        ["video1", "video3", "video2", "video4", "video5"],
    ]

    test_ground_truth = [
        {"video1", "video2"},
        {"video1", "video3"},
        {"video2", "video3"},
    ]

    test_latencies = [120.5, 115.3, 130.2]

    results = evaluate_search_system(
        ["query1", "query2", "query3"],
        test_retrieved,
        test_ground_truth,
        test_latencies,
    )

    print_evaluation_report(results)
