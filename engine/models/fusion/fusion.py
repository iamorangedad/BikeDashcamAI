"""
ç‰¹å¾èåˆæ¨¡å— - åŠ æƒèåˆ SigLIP å’Œ TimeSformer ç‰¹å¾
"""

import torch
import torch.nn as nn
import numpy as np


class FeatureFusion(nn.Module):
    """
    ç‰¹å¾èåˆæ¨¡å—

    å°† SigLIP å›¾åƒç‰¹å¾å’Œ TimeSformer æ—¶åºç‰¹å¾è¿›è¡ŒåŠ æƒèåˆ
    """

    def __init__(
        self,
        siglip_dim: int = 768,
        timesformer_dim: int = 768,
        output_dim: int = 512,
        fusion_method: str = "weighted_sum",
    ):
        """
        åˆå§‹åŒ–ç‰¹å¾èåˆæ¨¡å—

        Args:
            siglip_dim: SigLIP ç‰¹å¾ç»´åº¦
            timesformer_dim: TimeSformer ç‰¹å¾ç»´åº¦
            output_dim: è¾“å‡ºç‰¹å¾ç»´åº¦
            fusion_method: èåˆæ–¹æ³• ('weighted_sum', 'concat', 'attention')
        """
        super(FeatureFusion, self).__init__()

        self.siglip_dim = siglip_dim
        self.timesformer_dim = timesformer_dim
        self.output_dim = output_dim
        self.fusion_method = fusion_method

        # æŠ•å½±å±‚
        self.siglip_projection = nn.Linear(siglip_dim, output_dim)
        self.timesformer_projection = nn.Linear(timesformer_dim, output_dim)

        # å¯å­¦ä¹ çš„èåˆæƒé‡
        if fusion_method == "weighted_sum":
            self.alpha = nn.Parameter(torch.tensor(0.5))
            self.beta = nn.Parameter(torch.tensor(0.5))

        # æ³¨æ„åŠ›æœºåˆ¶
        elif fusion_method == "attention":
            self.attention = nn.MultiheadAttention(output_dim, num_heads=8)
            self.query = nn.Linear(output_dim, output_dim)
            self.key = nn.Linear(output_dim, output_dim)
            self.value = nn.Linear(output_dim, output_dim)

        # å±‚å½’ä¸€åŒ–
        self.layer_norm = nn.LayerNorm(output_dim)

    def forward(
        self, siglip_features: torch.Tensor, timesformer_features: torch.Tensor
    ) -> torch.Tensor:
        """
        å‰å‘ä¼ æ’­

        Args:
            siglip_features: SigLIP ç‰¹å¾ (batch_size, seq_len, siglip_dim)
            timesformer_features: TimeSformer ç‰¹å¾ (batch_size, seq_len, timesformer_dim)

        Returns:
            èåˆåçš„ç‰¹å¾ (batch_size, seq_len, output_dim)
        """
        # æŠ•å½±åˆ°ç›¸åŒç»´åº¦
        siglip_proj = self.siglip_projection(siglip_features)
        timesformer_proj = self.timesformer_projection(timesformer_features)

        # èåˆ
        if self.fusion_method == "weighted_sum":
            # åŠ æƒå’Œ
            alpha = torch.softmax(torch.stack([self.alpha, self.beta]), dim=0)
            fused = alpha[0] * siglip_proj + alpha[1] * timesformer_proj

        elif self.fusion_method == "concat":
            # æ‹¼æ¥åæŠ•å½±
            fused = torch.cat([siglip_proj, timesformer_proj], dim=-1)
            fused = nn.Linear(self.output_dim * 2, self.output_dim).to(fused.device)(
                fused
            )

        elif self.fusion_method == "attention":
            # æ³¨æ„åŠ›æœºåˆ¶
            query = self.query(siglip_proj)
            key = self.key(timesformer_proj)
            value = self.value(timesformer_proj)

            attn_output, _ = self.attention(query, key, value)
            fused = self.layer_norm(siglip_proj + attn_output)

        else:
            raise ValueError(f"æœªçŸ¥çš„èåˆæ–¹æ³•: {self.fusion_method}")

        return fused


class GlobalPooling(nn.Module):
    """
    å…¨å±€æ± åŒ–å±‚

    å°†åºåˆ—ç‰¹å¾æ± åŒ–ä¸ºå…¨å±€ç‰¹å¾å‘é‡
    """

    def __init__(self, pool_method: str = "mean"):
        """
        åˆå§‹åŒ–æ± åŒ–å±‚

        Args:
            pool_method: æ± åŒ–æ–¹æ³• ('mean', 'max', 'attention')
        """
        super(GlobalPooling, self).__init__()
        self.pool_method = pool_method

        if pool_method == "attention":
            self.attention_weights = nn.Linear(512, 1)

    def forward(self, features: torch.Tensor) -> torch.Tensor:
        """
        å‰å‘ä¼ æ’­

        Args:
            features: è¾“å…¥ç‰¹å¾ (batch_size, seq_len, feature_dim)

        Returns:
            å…¨å±€ç‰¹å¾å‘é‡ (batch_size, feature_dim)
        """
        if self.pool_method == "mean":
            # å¹³å‡æ± åŒ–
            pooled = features.mean(dim=1)

        elif self.pool_method == "max":
            # æœ€å¤§æ± åŒ–
            pooled = features.max(dim=1)[0]

        elif self.pool_method == "attention":
            # æ³¨æ„åŠ›æ± åŒ–
            attn_scores = torch.softmax(self.attention_weights(features), dim=1)
            pooled = (features * attn_scores).sum(dim=1)

        else:
            raise ValueError(f"æœªçŸ¥çš„æ± åŒ–æ–¹æ³•: {self.pool_method}")

        # L2 å½’ä¸€åŒ–
        pooled = pooled / (pooled.norm(dim=-1, keepdim=True) + 1e-8)

        return pooled


class VideoFeatureExtractor(nn.Module):
    """
    å®Œæ•´çš„è§†é¢‘ç‰¹å¾æå–å™¨

    åŒ…æ‹¬ç‰¹å¾èåˆå’Œå…¨å±€æ± åŒ–
    """

    def __init__(
        self,
        siglip_dim: int = 768,
        timesformer_dim: int = 768,
        output_dim: int = 512,
        fusion_method: str = "weighted_sum",
        pool_method: str = "attention",
    ):
        """
        åˆå§‹åŒ–ç‰¹å¾æå–å™¨

        Args:
            siglip_dim: SigLIP ç‰¹å¾ç»´åº¦
            timesformer_dim: TimeSformer ç‰¹å¾ç»´åº¦
            output_dim: è¾“å‡ºç‰¹å¾ç»´åº¦
            fusion_method: èåˆæ–¹æ³•
            pool_method: æ± åŒ–æ–¹æ³•
        """
        super(VideoFeatureExtractor, self).__init__()

        self.fusion = FeatureFusion(
            siglip_dim, timesformer_dim, output_dim, fusion_method
        )

        self.pooling = GlobalPooling(pool_method)

    def forward(
        self, siglip_features: torch.Tensor, timesformer_features: torch.Tensor
    ) -> torch.Tensor:
        """
        å‰å‘ä¼ æ’­

        Args:
            siglip_features: SigLIP ç‰¹å¾
            timesformer_features: TimeSformer ç‰¹å¾

        Returns:
            å…¨å±€è§†é¢‘ç‰¹å¾å‘é‡
        """
        # ç‰¹å¾èåˆ
        fused = self.fusion(siglip_features, timesformer_features)

        # å…¨å±€æ± åŒ–
        global_feature = self.pooling(fused)

        return global_feature


def export_fusion_to_onnx(
    output_path: str = "fusion.onnx",
    siglip_dim: int = 768,
    timesformer_dim: int = 768,
    output_dim: int = 512,
):
    """
    å¯¼å‡ºèåˆæ¨¡å‹åˆ° ONNX

    Args:
        output_path: è¾“å‡ºè·¯å¾„
        siglip_dim: SigLIP ç‰¹å¾ç»´åº¦
        timesformer_dim: TimeSformer ç‰¹å¾ç»´åº¦
        output_dim: è¾“å‡ºç‰¹å¾ç»´åº¦
    """
    print("ğŸ“¦ åˆå§‹åŒ–ç‰¹å¾èåˆæ¨¡å‹...")
    model = VideoFeatureExtractor(
        siglip_dim=siglip_dim,
        timesformer_dim=timesformer_dim,
        output_dim=output_dim,
        fusion_method="weighted_sum",
        pool_method="mean",
    )
    model.eval()

    # å‡†å¤‡ç¤ºä¾‹è¾“å…¥
    batch_size = 4
    seq_len = 16

    dummy_siglip = torch.randn(batch_size, seq_len, siglip_dim)
    dummy_timesformer = torch.randn(batch_size, seq_len, timesformer_dim)

    print(f"ğŸš€ å¯¼å‡ºèåˆæ¨¡å‹åˆ° {output_path}...")

    torch.onnx.export(
        model,
        (dummy_siglip, dummy_timesformer),
        output_path,
        input_names=["siglip_features", "timesformer_features"],
        output_names=["global_feature"],
        dynamic_axes={
            "siglip_features": {0: "batch_size", 1: "seq_len"},
            "timesformer_features": {0: "batch_size", 1: "seq_len"},
            "global_feature": {0: "batch_size"},
        },
        opset_version=17,
    )

    print(f"âœ… èåˆæ¨¡å‹å·²å¯¼å‡ºåˆ° {output_path}")
    return output_path


if __name__ == "__main__":
    # å¯¼å‡ºåˆ° ONNX
    export_fusion_to_onnx()

    # æµ‹è¯•æ¨ç†
    print("\nğŸ§ª æµ‹è¯•ç‰¹å¾èåˆ...")
    fusion_model = VideoFeatureExtractor()

    siglip_feat = torch.randn(2, 16, 768)
    timesformer_feat = torch.randn(2, 16, 768)

    with torch.no_grad():
        global_feat = fusion_model(siglip_feat, timesformer_feat)

    print(f"âœ… èåˆç‰¹å¾å½¢çŠ¶: {global_feat.shape}")
    print(f"   ç‰¹å¾èŒƒæ•°: {global_feat.norm(dim=-1)}")
