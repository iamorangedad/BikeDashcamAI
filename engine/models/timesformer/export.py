"""
TimeSformer æ¨¡å‹å¯¼å‡ºä¸º TensorRT å¼•æ“
"""

import torch
from transformers import TimesformerModel, TimesformerImageProcessor
import tensorrt as trt
import numpy as np


def export_timesformer_to_onnx(
    output_path: str = "timesformer.onnx", num_frames: int = 16
):
    """
    å¯¼å‡º TimeSformer åˆ° ONNX æ ¼å¼

    Args:
        output_path: è¾“å‡º ONNX æ–‡ä»¶è·¯å¾„
        num_frames: è¾“å…¥è§†é¢‘å¸§æ•°
    """
    print("ğŸ“¦ åŠ è½½ TimeSformer æ¨¡å‹...")
    model = TimesformerModel.from_pretrained("facebook/timesformer-base-finetuned-k400")
    model.eval()

    processor = TimesformerImageProcessor.from_pretrained(
        "facebook/timesformer-base-finetuned-k400"
    )

    # å‡†å¤‡ç¤ºä¾‹è¾“å…¥: (batch_size, num_frames, channels, height, width)
    dummy_video = torch.randn(1, num_frames, 3, 224, 224)

    print(f"ğŸš€ å¯¼å‡º TimeSformer åˆ° {output_path}...")

    torch.onnx.export(
        model,
        dummy_video,
        output_path,
        input_names=["pixel_values"],
        output_names=["last_hidden_state", "pooler_output"],
        dynamic_axes={
            "pixel_values": {0: "batch_size", 1: "num_frames"},
            "last_hidden_state": {0: "batch_size", 1: "num_frames"},
            "pooler_output": {0: "batch_size"},
        },
        opset_version=17,
        do_constant_folding=True,
    )

    print(f"âœ… TimeSformer å·²å¯¼å‡ºåˆ° {output_path}")
    return output_path


def convert_onnx_to_tensorrt(
    onnx_path: str,
    engine_path: str = "timesformer.plan",
    fp16_mode: bool = True,
    num_frames: int = 16,
):
    """
    å°† ONNX æ¨¡å‹è½¬æ¢ä¸º TensorRT å¼•æ“

    Args:
        onnx_path: ONNX æ¨¡å‹è·¯å¾„
        engine_path: è¾“å‡º TensorRT å¼•æ“è·¯å¾„
        fp16_mode: æ˜¯å¦ä½¿ç”¨ FP16 ç²¾åº¦
        num_frames: è¾“å…¥è§†é¢‘å¸§æ•°
    """
    TRT_LOGGER = trt.Logger(trt.Logger.INFO)

    print("ğŸ”§ åˆå§‹åŒ– TensorRT...")
    builder = trt.Builder(TRT_LOGGER)
    network = builder.create_network(
        1 << int(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH)
    )
    parser = trt.OnnxParser(network, TRT_LOGGER)

    print(f"ğŸ“– è¯»å– ONNX æ¨¡å‹: {onnx_path}")
    with open(onnx_path, "rb") as model:
        if not parser.parse(model.read()):
            print("âŒ ONNX è§£æå¤±è´¥:")
            for error in range(parser.num_errors):
                print(parser.get_error(error))
            return

    print("âœ… ONNX æ¨¡å‹è§£ææˆåŠŸ")

    # é…ç½® TensorRT
    config = builder.create_builder_config()

    if fp16_mode and builder.platform_has_fast_fp16:
        config.set_flag(trt.BuilderFlag.FP16)
        print("âš¡ å¯ç”¨ FP16 æ¨¡å¼")

    # è®¾ç½®ä¼˜åŒ–é…ç½®æ–‡ä»¶
    profile = builder.create_optimization_profile()

    # è®¾ç½®è¾“å…¥ç»´åº¦èŒƒå›´
    min_shape = (1, 1, 3, 224, 224)
    opt_shape = (1, num_frames, 3, 224, 224)
    max_shape = (4, 32, 3, 224, 224)

    profile.set_shape("pixel_values", min_shape, opt_shape, max_shape)
    config.add_optimization_profile(profile)

    # è®¾ç½®æœ€å¤§å·¥ä½œç©ºé—´
    config.max_workspace_size = 1 << 31  # 2GB

    print("ğŸ”¨ æ„å»º TensorRT å¼•æ“...")
    serialized_engine = builder.build_serialized_network(network, config)

    if not serialized_engine:
        print("âŒ TensorRT å¼•æ“æ„å»ºå¤±è´¥")
        return

    print(f"ğŸ’¾ ä¿å­˜ TensorRT å¼•æ“åˆ° {engine_path}")
    with open(engine_path, "wb") as f:
        f.write(serialized_engine)

    print("âœ… TensorRT å¼•æ“æ„å»ºå®Œæˆ")
    return engine_path


def test_inference(engine_path: str, input_video: np.ndarray):
    """
    æµ‹è¯• TensorRT æ¨ç†

    Args:
        engine_path: TensorRT å¼•æ“è·¯å¾„
        input_video: è¾“å…¥è§†é¢‘æ•°ç»„ (batch_size, num_frames, channels, height, width)
    """
    print(f"ğŸ§ª æµ‹è¯•æ¨ç†: {engine_path}")

    TRT_LOGGER = trt.Logger(trt.Logger.INFO)

    # åŠ è½½å¼•æ“
    with open(engine_path, "rb") as f:
        engine = trt.Runtime(TRT_LOGGER).deserialize_cuda_engine(f.read())

    # åˆ›å»ºæ‰§è¡Œä¸Šä¸‹æ–‡
    context = engine.create_execution_context()

    # å‡†å¤‡è¾“å…¥è¾“å‡ºç¼“å†²åŒº
    inputs = [input_video]
    outputs = []
    bindings = []

    for i in range(engine.num_io_tensors):
        name = engine.get_tensor_name(i)
        dtype = trt.nptype(engine.get_tensor_dtype(name))

        if engine.get_tensor_mode(name) == trt.TensorIOMode.INPUT:
            # è®¾ç½®åŠ¨æ€ç»´åº¦
            context.set_input_shape(name, inputs[0].shape)
            shape = inputs[0].shape
            bindings.append(inputs[i].astype(dtype).reshape(shape))
        else:
            shape = context.get_tensor_shape(name)
            output = np.empty(shape, dtype=dtype)
            outputs.append(output)
            bindings.append(output)

    # æ‰§è¡Œæ¨ç†
    print("ğŸš€ æ‰§è¡Œæ¨ç†...")
    for i in range(engine.num_io_tensors):
        context.set_tensor_address(engine.get_tensor_name(i), bindings[i].ctypes.data)

    context.execute_async_v3(0)

    print(f"âœ… æ¨ç†å®Œæˆ")
    for i, output in enumerate(outputs):
        print(f"   è¾“å‡º {i} å½¢çŠ¶: {output.shape}")

    return outputs


if __name__ == "__main__":
    # å¯¼å‡ºåˆ° ONNX
    onnx_path = export_timesformer_to_onnx()

    # è½¬æ¢ä¸º TensorRT
    engine_path = convert_onnx_to_tensorrt(onnx_path)

    # æµ‹è¯•æ¨ç†
    dummy_input = np.random.randn(1, 16, 3, 224, 224).astype(np.float32)
    test_inference(engine_path, dummy_input)
